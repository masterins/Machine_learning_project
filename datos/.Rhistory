oauth_endpoints("github")
# 2. Register an application at https://github.com/settings/applications
#    Insert your values below - if secret is omitted, it will look it up in
#    the GITHUB_CONSUMER_SECRET environmental variable.
#
#    Use http://localhost:1410 as the callback url
myapp <- oauth_app("github", "56b637a5baffac62cad9")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. Register an application at https://github.com/settings/applications
#    Insert your values below - if secret is omitted, it will look it up in
#    the GITHUB_CONSUMER_SECRET environmental variable.
#
#    Use http://localhost:1410 as the callback url
myapp <- oauth_app("github", "56b637a5baffac62cad9")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", "0b81ead0b9bb9070206693abd5a3c7101f07a0a6")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
oauth_endpoints("github")
myapp <- oauth_app("github", "60c4635d70e2e821010dc150653f78cf6b678b0f")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
oauth_endpoints("https://api.github.com/users/jtleek/repos")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
p
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
install.packages("ggplot2")
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library (ggplot2)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality, geom = "smooth")
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
g
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot (votos, rating, data = películas, el panel = panel.loess)
qplot (votos, clasificación, datos = vídeos) + stats_smooth ("loess")
qplot (votos, rating, data = películas, suave = "loess")
qplot (votos, rating, data = películas)
library(ggplot2)
qplot(votes, rating, data = movies, panel = panel.loess)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies, smooth = "loess")
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
m
apply(m, 1, mean)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=10, ncol=3)
m
apply(m, 1, mean)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=4, ncol=3)
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=5, ncol=3)
m
apply(m, 1, mean)
apply(m, mean)
lapply(m, mean)
x<-list(a=1:5, b=rnorm(10))
x
lapply(x,mean)
lapply(x,1,mean)
# Create 2 vectors normally distributed but with different units
x <- rnorm(1000, 200, 20)
y <- rnorm(1000, 5, 0.1)
# Normalize x
mean.x <- mean(x)
stdv.x <- sd(x)
z.x <- (x - mean.x) / stdv.x
# Normalize y
mean.y <- mean(y)
stdv.y <- sd(y)
z.y <- (y - mean.y) / stdv.y
# Get some summary statistics
summary(x)
summary(z.x)
summary(y)
summary(z.y)
# Create some histograms
par(mfrow = c(2, 2))
hist(x)
hist(z.x)
hist(y)
hist(z.y)
# Create the qqplots (just for fun)
par(mfrow = c(1, 2))
qqplot(x,y)
qqplot(z.x,z.y)
round(ppois(20,lambda=16.5*2)*100,1)
round(ppois(10,lambda=5*3)*100,2)
round(ppois(10,lambda=5*3),2)
round(qnorm(.95,mean=100,sd=10),3)
qnorm(.95,mean=1100,sd=75)
qnorm(.95,mean=1100,sd=75/sqrt(100))
pnorm(.51,mean=0.5,sd=sqrt(1/2/100),lower.tail=false)
round(pnorm(.51,mean=0.5,sd=sqrt(1/2/100),lower.tail=false),3)
round(pnorm(.51,mean=0.5,sd=sqrt(1/2/100),lower.tail=FALSE),3)
pnorm(.5,mean=0.5,sd=sqrt(1/2/1000),lower.tail=FALSE)
pnorm(.5,mean=0.5,sd=sqrt(1/12/1000),lower.tail=FALSE)
pnorm(.5,mean=0.5,sd=sqrt(1/12/100),lower.tail=FALSE)
pnorm(.5,mean=1000,sd=sqrt(1/12/100),lower.tail=FALSE)
pnorm(.5,mean=100,sd=sqrt(1/12/1000),lower.tail=FALSE)
pnorm(.5,mean=100,sd=sqrt(1/12/100),lower.tail=FALSE)
pnorm(.5,mean=1000,sd=sqrt(1/12/100),lower.tail=FALSE)
unif100mean <- function(unused){
return(mean(runif(100,min=0,max=1)))
}
sd(sapply(1:1000,unif100mean))
mn + c(-1, 1) * qt(.975, n - 1) * s / sqrt(n)
(qt(0.975, df=8) - qt(0.025, df=8)) * 0.30 / sqrt(0.95)
(qt(0.975, df=8) - qt(0.025, df=8)) * 0.30 / sqrt(9)
install.packages("swirl")
install.packages("UsingR")
library(UsingR)
data(galtron)
data(galton)
galton
par(mfrow=(1,2))
par(mfrow=c(1,2))
galton
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="red",breaks=100)
library(manipulate)
library(UsingR)
data(galton)
myhist<-function(mu)
{
hist(galton$child,col="blue",breaks=100)
lines(c(mu,mu),c(0,150),col="red",lwd=5)
mse<-mean((galton$child-mu)^2)
text(63,150,paste("mu=",mu))
text(63,140,paste("MSE=",round(mse,2)))
}
source('C:/Users/masterins/Desktop/linear regresion/semana 1/ejemplos/manipulate_minimos_cuadrados.R')
source('C:/Users/masterins/Desktop/linear regresion/semana 1/ejemplos/manipulate_minimos_cuadrados.R')
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
source('C:/Users/masterins/Desktop/linear regresion/semana 1/ejemplos/manipulate_minimos_cuadrados.R')
source('C:/Users/masterins/Desktop/linear regresion/semana 1/ejemplos/manipulate_minimos_cuadrados.R')
source('C:/Users/masterins/Desktop/linear regresion/semana 1/ejemplos/manipulate_minimos_cuadrados.R')
hist(galton$child,col="blue",breaks=100)
lines(c(mu,mu),c(0,150),col="red",lwd=5)
mse<-mean((galton$child-mu)^2)
text(63,150,paste("mu=",mu))
text(63,140,paste("MSE=",round(mse,2)))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
lm(x~1)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
xn <- (x - mean(x))/sd(x)
xn[1]
library(datasets)
data(mtcars)
mtcars$mpg
yc <- mtcars$mpg - mean(mtcars$mpg)
xc <- mtcars$wt - mean(mtcars$wt)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(mtcars$mpg ~ mtcars$wt))[2])
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(w*x)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(x~y)
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])
library(datasets)
data(mtcars)
mtcars$mpg
yc <- mtcars$mpg - mean(mtcars$mpg)
xc <- mtcars$wt - mean(mtcars$wt)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(mtcars$mpg ~ mtcars$wt))[2])
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)
library(caret)
install.packages("caret")
install.packages("spam")
library(kernlab)
data(spam)
spam
intrain<-createDataPartition(y=spam$type, p=0.75,list=false)
inTrain<-createDataPartition(y=spam$type, p=0.75,list=FALSE)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y~x)
summary(fit)
x<-mtcars$wt
y<-mtcars$mpg
predict(lm(y~x) , newdata = data.frame(x=mean(x)), interval="confidence")
predict(fit,data.frame(x=3),interval=("prediction"))
x<-mtcars$wt*0.5
fit<-lm(y~x)
#predict(fit,data.frame(x=3),interval=("confidence"))
sumCoef<-summary(fit)$coefficients
sumCoef[2,1]+c(-1,1)*qt(.975,df=fit$df)*sumCoef[2,2]
# notes
e<-resid(fit)
yhat<-predict(fit)
max(abs(e-(y-yhat)))
b1<-coef(fit)[2]
# alternative
# b1<-cor(y,x)*sd(y)/sd(x)
b0<-coef(fit)[1]
# alternative
# b0<-mean(y)-b1*mean(x)
e<-y-b0-b1*x
sigma<-sqrt(sum(e^2)/(n-2))
# sum of squared Xs
ssx<-sum((x-mean(x))^2)
ssx<-sum((y-mean(y))^2)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y~x)
summary(fit)
?mtcars
ssx <- sum((x - mean(x))^2)
n <- length(x)
intercept + slope * 3 + c(-1,1) * qt(0.975, df.residual) * sigma * sqrt(1 + 1/n + (3 - mean(x))^2 / ssx) #prediction interval
predict(fit, data.frame(x=3), interval = ("prediction")) #same
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)$coefficients[2,4] #p-value
#q2
# Consider the previous problem, give the estimate of the residual standard deviation.
summary(fit)$sigma #sigma. really
#q3
# In the mtcars data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint?
data(mtcars)
x <- mtcars$wt
y <- mtcars$mpg
fit <- lm(y ~ x)
intercept <- fit$coeff[1]
slope <- fit$coeff[2]
df.residual <- fit$df.residual
sigma <- summary(fit)$sigma
plot(x, y)
abline(fit)
xmean <- mean(x)
yest <- intercept + slope * xmean
yest + c(-1,1) * qt(0.975, df.residual) * sigma / sqrt(length(x)) #confidence interval
predict(fit, data.frame(x=mean(x)), interval = ("confidence")) #obtains the same value
#q4
# Refer to the previous question. Read the help file for mtcars. What is the weight coefficient interpreted as?
?mtcars
#q5
# Consider again the mtcars data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint?
ssx <- sum((x - mean(x))^2)
n <- length(x)
intercept + slope * 3 + c(-1,1) * qt(0.975, df.residual) * sigma * sqrt(1 + 1/n + (3 - mean(x))^2 / ssx) #prediction interval
predict(fit, data.frame(x=3), interval = ("prediction")) #same
#q6
# Consider again the mtcars data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint.
sumCoef <- summary(fit)$coeff
2 * (sumCoef[2,1] + c(-1, 1) * qt(0.975, df.residual) * sumCoef[2,2])
#q9
# Refer back to the mtcars data set with mpg as an outcome and weight (wt) as the predictor. About what is the ratio of the the sum of the squared errors, sum((y-mean(y))^2)  when comparing a model with just an intercept (denominator) to the model with the intercept and slope (numerator)?
e1 <- y - intercept
e2 <- y - intercept - slope * x
sum(e2^2) / sum(e1^2)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)$coefficients[2,4] #p-value
#q2
# Consider the previous problem, give the estimate of the residual standard deviation.
summary(fit)$sigma #sigma. really
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
library(Hmisc)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
data(mtcars)
attach(mtcars)
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/submission_project_script.R')
library(lattice)
library(ggplot2)
library(graphics)
library(Hmisc)
library(e1071)
library(caret)
library(ipred)
library(RANN)
library(randomForest)
set.seed(1234)
trainingCSV = read.csv("pml-training.csv")
testingCSV =  read.csv("pml-testing.csv")
inTrain <- createDataPartition(trainingCSV$classe, p=0.70, list=FALSE)
training <- trainingCSV[inTrain, ]
validation <- trainingCSV[-inTrain, ]
testing<-testingCSV
goodVar<-c((colSums(is.na(training[,-160])) >= 0.4*nrow(training)),160)
training<-training[,goodVar]
dim(training)
validation<-validation[,goodVar]
dim(training)
testing<-testing[,goodVar]
rf <- train(classe ~ ., data= training,method = "rf",preProcess = c("center", "scale","knnImpute","pca"))
confusionMatrix(predict(rf, verification[,-verification$classe],verification$classe))
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/submission_project_script.R')
.Rhistory
# Example code in package build process
*-Ex.R
# R data files from past sessions
.Rdata
library(caret)
trainingOri <- read.csv("~/R resources/Course Project/pml-training.csv", header=TRUE, na.strings = c("NA",""))
testingOri <- read.csv("~/R resources/Course Project/pml-testing.csv", header=TRUE, na.strings = c("NA",""))
dim(trainingOri)
summary(trainingOri)
#Remove variables with almost zero variances
nzv<-nearZeroVar(trainingOri)
trainingOri2 <- trainingOri[,-nzv]
testingOri2 <-testingOri[,-nzv]
dim(trainingOri2)
#Remove varibles that are mostly 'NA'
trainingOri3<-trainingOri2[,colSums(is.na(trainingOri2))==0]
testingOri3<-testingOri2[,colSums(is.na(trainingOri2))==0]
dim(trainingOri3)
summary(trainingOri3)
#Remove variables that are non-numeric or (apparently) non-informative
trainingOriFin<-trainingOri3[,c(-1,-2,-3,-5)]
testingOriFin<-testingOri3[,c(-1,-2,-3,-5)]
dim(trainingOriFin)
#Data splitting
inTrain <- createDataPartition(y=trainingOriFin$classe,p=0.6,list=FALSE)
training <- trainingOriFin[inTrain,]
testing<-trainingOriFin[-inTrain,]
dim(training)
dim(testing)
#Check Correlation
M<-abs(cor(training[,-55]))
diag(M)<-0
head(which(M>0.8,arr.ind=T))
#Check quality of highly correlated variables
library(Hmisc)
cutyb<-cut2(training$yaw_belt,g=5)
qplot(c(1:11776),training$classe,color=cutyb)
#Model Building
set.seed(726)
#Model 1: use original set
modelFit <- train(training$classe ~.,data=training,method="rf",trControl = trainControl(method='cv'))
confusionMatrix(testing$classe,predict(modelFit,testing))
#Model 2: remove highly correlated and low contribution predictors
training2 <- training[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
testing2 <- testing[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
modelFit2 <- train(training$classe ~.,data=training2,method="rf",trControl = trainControl(method='cv'))
confusionMatrix(testing$classe,predict(modelFit2,testing2))
#Model 3: use PCA to reduce correlation
preProc <- preProcess(training[,-55],method="pca",thresh=0.95)
trainPC <- predict(preProc,training[,-55])
modelFit3 <- train(training$classe ~.,data=trainPC,method="rf",trControl = trainControl(method='cv'))
testPC <- predict(preProc,testing[,-55])
confusionMatrix(testing$classe,predict(modelFit3,testPC))
#Predict with Model 2 using testing set
testingOriFin2<-testingOriFin[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
answers <- predict(modelFit2,testingOriFin2)
answers
#File output
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/submission_project_script.R')
lm(mpg ~ factor(cyl)+wt, data = mtcars)
lm(mpg ~ factor(cyl), data = mtcars)
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
install.packages("lmtest")
lrtest(l2,l1)
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit)$hat
dfbetas(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata
install.packages("tree")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata)
install.packages("pgmm")
install.packages("caret")
install.packages("confusionmatrix")
install.packages("randomForest")
install.packages("confusionMatrix")
actual    <- sample(1:4, 1000, replace=TRUE)
predicted <- sample(c(1L,2L,4L), 1000, replace=TRUE)
print(ConfusionMatrix(actual, predicted) )
print(confusionMatrix(actual, predicted) )
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
clear
clear()
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
source('C:/Users/masterins/Desktop/Data Science Johns Hopkins University/Machine learning/Project/project_test.R')
